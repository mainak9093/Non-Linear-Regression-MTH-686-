---
title: "**Non Linear Regression MTH 686**"
author: "Mainak Sarkar | 230619"
output:
  pdf_document:
    latex_engine: xelatex
    pandoc_args:
      - --variable=geometry:margin=1in
header-includes:
  - \usepackage{titling}
  - \setlength{\droptitle}{-10em}
  - \setlength{\parskip}{0.01pt}
  - \setlength{\headheight}{4pt}
  - \setlength{\textheight}{690pt}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results = "hide", warning = FALSE, message = FALSE)
library(MASS)
```

```{r load-data}
# Load data
data <- read.table("set-49.dat")
head(data)

# Extract time and observed values
t_orig <- as.numeric(unlist(data[1]))  # Original time (1-60)
y1 <- as.numeric(unlist(data[2]))      # Observed y(t) values
n <- length(y1)
```
# Model 1: Exponential Components Model

$y(t) = \alpha_0 + \alpha_1 e^{\beta_1 t} + \alpha_2 e^{\beta_2 t} + e(t)$, where $e(t) \sim N(0, \sigma^2)$, for all t.

## 1.1 Theory: Non-Linear Least Squares (NLS)

This model is non-linear in its parameters (specifically $\beta_1$ and $\beta_2$). The original Prony's method, which linearizes the problem, is highly unstable with large $t$ values. Therefore, we must estimate all 5 parameters simultaneously using a robust numerical optimization method to find the parameters that minimize the Residual Sum of Squares (RSS).

## 1.2 Assumptions

- **Normality of Errors:** The model assumes the error term $e(t)$ is i.i.d. normal with mean zero and constant variance $\sigma^2$.
- **Model Structure:** The data is assumed to be well-described by a constant plus two exponential terms.

## 1.3 Estimation Algorithm

The parameters are estimated using R's `optim` function to perform Non-Linear Least Squares (NLS):

1. **Model & Cost Functions:** A function for the model, `f_model_1(p, t)`, and a Sum of Squared Errors (SSE) cost function, `sse_1(p)`, are defined. $p$ is a vector containing all 5 parameters.

2. **Initial Guesses:** The `optim` function requires initial guesses. Based on an analysis of the data and the sample report, starting values are chosen to give the optimizer a reasonable starting point.

3. **Optimization:** The "L-BFGS-B" method within `optim` is used. This is a quasi-Newton method that iteratively searches the 5-dimensional parameter space to find the combination of parameters that minimizes the SSE.

```{r model1-fit}
# --- MODEL 1: NLS implementation ---

# Use original time values
tt_m1 <- t_orig
yy_m1 <- y1

# Model 1 function: p = c(a0, a1, b1, a2, b2)
f_model_1 <- function(p, t) {
  a0 <- p[1]; a1 <- p[2]; b1 <- p[3]; a2 <- p[4]; b2 <- p[5]
  return(a0 + a1 * exp(b1 * t) + a2 * exp(b2 * t))
}

# SSE cost function for Model 1
sse_1 <- function(p) {
  yhat <- f_model_1(p, tt_m1)
  if (any(!is.finite(yhat))) return(1e10) # Penalize explosions
  sum((yy_m1 - yhat)^2)
}

# Initial guesses: c(a0, a1, b1, a2, b2)
init_1 <- c(1.0, 0.2, 0.001, 2.0, -0.3) # Inspired by sample PDF

# Run optim
fit_1 <- optim(init_1, sse_1, method = "L-BFGS-B", 
               control = list(maxit = 2000, fnscale = 1e-1))

params_1 <- fit_1$par
parameter_estimates_1 <- params_1

alpha0_1 <- params_1[1]
alpha1_1 <- params_1[2]
beta1_1 <- params_1[3]
alpha2_1 <- params_1[4]
beta2_1 <- params_1[5]

fitted_values_m1 <- f_model_1(params_1, tt_m1)
residuals_m1 <- yy_m1 - fitted_values_m1
```

## Answers to the Questions for Model 1

### Ans 1.1: Least Squares Estimators under Model 1

The estimated parameters for the Exponential Components Model are: **\(\hat{\alpha_0} = 0.5441275\), \(\hat{\alpha_1} = -354.7214\), \(\hat{\alpha_2} = 4.10648\), \(\hat{\beta_1} = -6.483438\), and \(\hat{\beta_2} = -0.3207875\)**, derived under the assumptions of normality, independence, and linearity of the error terms.
These estimators were derived under the assumptions of normality, independence, and linearity of the error terms.

### Ans 1.2: Estimation Methodology for Least Squares Estimators

The parameters were estimated using a Non-Linear Least Squares (NLS) approach with the `optim` function (L-BFGS-B method). This was necessary because methods like Prony's are numerically unstable with the original $t$ (1-60) values. Initial guesses were provided, and the optimizer iteratively minimized the Sum of Squared Errors (SSE) to find the final 5 parameters.

### Ans 1.3 Best Model : (See Ans 3.3)

```{r model1-accuracy}
# --- Model 1 Accuracy ---
rmse_1 <- sqrt(mean((residuals_m1)^2, na.rm = TRUE))
rss_1 <- sum((residuals_m1)^2)                  
tss_1 <- sum((y1 - mean(y1))^2)  
r_squared_1 <- 1 - (rss_1 / tss_1)
k_1 <- 5
adjusted_r_squared_1 <- 1 - ((1 - r_squared_1) * (n - 1) / (n - k_1 - 1))
```

### Ans 1.4: Estimate of $\sigma^2$

The estimate of the error variance $\sigma^2$ is given by $\hat{\sigma}^2 = \frac{RSS}{n - k}$, where $n=60$ and $k=5$ (the number of estimated parameters).The computed value of \(\hat{\sigma}^2\) is **0.0596968.**

```{r model1-fim-ci}
# Define the Jacobian of the model (partial derivatives)
jacobian_1 <- function(t, alpha0, alpha1, beta1, alpha2, beta2) {
  d_alpha0 <- 1 
  d_alpha1 <- exp(beta1 * t)  
  d_beta1 <- alpha1 * t * exp(beta1 * t) 
  d_alpha2 <- exp(beta2 * t)  
  d_beta2 <- alpha2 * t * exp(beta2 * t)  
  
  return(c(d_alpha0, d_alpha1, d_beta1, d_alpha2, d_beta2))
}

# Fisher Information matrix calculation (5-param version)
fisher_information <- function(t_vec, params, jac_func, sigma2) {
  n_obs <- length(t_vec)
  n_params <- length(params)
  J <- matrix(0, nrow = n_obs, ncol = n_params)
  
  for (i in 1:n_obs) {
    J[i, ] <- jac_func(t_vec[i], params[1], params[2], params[3], params[4], params[5])
  }
  
  fisher_matrix <- (1 / (n_obs * sigma2)) * (t(J) %*% J)
  lambda <- 1e-6
  fisher_matrix_regularized <- fisher_matrix + lambda * diag(n_params)
  
  return(fisher_matrix_regularized)
}

# Confidence Intervals calculation (5-param version)
confidence_intervals <- function(fisher_matrix, parameter_estimates, alpha = 0.05) {
  fisher_inv <- ginv(fisher_matrix)
  se <- sqrt(diag(fisher_inv))
  z_value <- qnorm(1 - alpha / 2)
  ci_lower <- parameter_estimates - z_value * se
  ci_upper <- parameter_estimates + z_value * se
  
  return(list(lower = ci_lower, upper = ci_upper, se = se))
}

sigma2_1 <- rss_1 / (n - k_1)

# Calculate Fisher Information Matrix for the model parameters
fisher_matrix_1 <- fisher_information(tt_m1, parameter_estimates_1, jacobian_1, sigma2_1)

# Calculate Confidence Intervals
ci_1 <- confidence_intervals(fisher_matrix_1, parameter_estimates_1)

# Print the results
cat("--- Model 1 Results ---\n")
cat("Parameters (alpha0, alpha1, beta1, alpha2, beta2):\n")
print(parameter_estimates_1)
cat("\nSigma^2 Estimate:\n")
print(sigma2_1)
cat("\nFisher Information Matrix:\n")
print(fisher_matrix_1)
cat("\nConfidence Intervals:\n")
print(ci_1)
```

### Ans 1.5: Confidence Intervals based on the Fisher Information Matrix

**Jacobian Matrix:**

$$J = \begin{bmatrix} \frac{\partial f}{\partial \alpha_0} & \frac{\partial f}{\partial \alpha_1} & \frac{\partial f}{\partial \beta_1} & \frac{\partial f}{\partial \alpha_2} & \frac{\partial f}{\partial \beta_2} \end{bmatrix}$$
where the partial derivatives are: $1$, $e^{\beta_1 t}$, $\alpha_1 t e^{\beta_1 t}$, $e^{\beta_2 t}$, and $\alpha_2 t e^{\beta_2 t}$.
2. **Fisher Information Matrix (FIM)**  
   The Fisher Information Matrix (FIM) is computed as the sum of outer products of the Jacobian at each data point:

   $$
   \begin{bmatrix}
    16.751317140 & 4.274060e-04 & -0.1518421764 & 0.7381788325 & 11.046160993 \\
    0.000427406 & 1.652313e-06 & -0.0002313899 & 0.0003099859 & 0.001274364 \\
    -0.151842176 & -2.313899e-04 & 0.0820803343 & -0.1100807424 & -0.453048135 \\
    0.738178833 & 3.099859e-04 & -0.1100807424 & 0.3103932703 & 2.691697567 \\
    11.046160993 & 1.274364e-03 & -0.4530481353 & 2.6916975668 & 35.630982867
  \end{bmatrix}
   $$

### 3. Confidence Intervals  
The 95% confidence intervals for each parameter are:

**\( \alpha_0 \)**: \( [-3.2058 \times 10^{-4},\ 1.0886] \),  **\( \alpha_1 \)**: \( [-2314.68,\ 1605.23] \),  **\( \alpha_2 \)**: \( [-20.7854,\ 7.8185] \),  **\( \beta_1 \)**: \( [-7.0635,\ 15.2765] \),  **\( \beta_2 \)**: \( [-1.1435,\ 0.5019] \).  
The corresponding standard errors are:  
**SE(\( \alpha_0 \))** = 0.2778,  **SE(\( \alpha_1 \))** = 999.996,  **SE(\( \alpha_2 \))** = 7.2970,  **SE(\( \beta_1 \))** = 5.6991,  **SE(\( \beta_2 \))** = 0.4198.


### Ans 1.6 - 1.8: Residual Diagnostics and Model Fit

```{r model1-plots, fig.width=6, fig.height=4.5}
# Set up the plotting area to have two rows
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Residuals plot
plot(t_orig, residuals_m1, main = "1.6: Residuals vs. Time (Model 1)",
     xlab = "Time (Original)", ylab = "Residuals", col = "black", pch = 19)
abline(h = 0, col = "red")

# Q-Q plot of residuals
qqnorm(residuals_m1, main = "1.7(a): Q-Q Plot of Residuals (Model 1)")
qqline(residuals_m1, col = "red")

# Histogram of residuals
hist(residuals_m1, breaks = 10, main = "1.7(b): Histogram of Residuals (Model 1)",
     xlab = "Residuals", col = "lightblue", border = "black")

# Fitted values plot
plot(t_orig, y1, type = "l", col = "blue", main = "1.8: Observed vs. Fitted Data (Model 1)",
     xlab = "Time (Original)", ylab = "Values", lwd = 2)
lines(t_orig, fitted_values_m1, col = "red", lwd = 2)
legend("topright", legend = c("Observed Data", "Model-1 Fit"), 
       col = c("blue", "red"), lty = 1, lwd = 2)

par(mfrow = c(1, 1))
```

```{r model1-ks-test, echo=FALSE,results='hide'}
# --- Model 1 KS Test ---
standardized_residuals_1 <- scale(residuals_m1)
ks_test_1 <- ks.test(standardized_residuals_1, "pnorm", mean = 0, sd = 1)

cat("**Kolmogorov-Smirnov Test for Model 1:**\n\n")
cat("KS Test Statistic:", sprintf("%.6f", ks_test_1$statistic), "\n\n")
cat("KS Test p-value:", sprintf("%.6f", ks_test_1$p.value), "\n\n")
if (ks_test_1$p.value < 0.05) {
  cat("**Conclusion:** With p-value < 0.05, we reject the normality assumption for Model 1 residuals.\n")
} else {
  cat("**Conclusion:** With p-value ≥ 0.05, we fail to reject the normality assumption for Model 1 residuals.\n")
}
```
- **1.6: Residual Spread**: Residuals are well-centered around zero, indicating minimal bias.

- **1.7: Kolmogorov-Smirnov Test**: With a test statistic of 0.087556 and p-value of 0.713978, there’s no significant deviation from normality.

- **1.7(a & b): Q-Q Plot and Histogram**: Both confirm normality, as residuals align closely with a normal distribution.

- **1.8: Fitted Line**: The fitted line closely matches the observed data's central trend, suggesting a good fit.

# Model 2: Nonlinear Ratio Model

\( y(t) = \frac{\alpha_0 + \alpha_1 \cdot t}{\beta_0 + \beta_1 \cdot t} + e(t) \), where \( e(t) \sim N(0, \sigma^2) \) for all \( t \).

where $e(t) \sim N(0, \sigma^2)$ for all $t$.

## 2.1 Theory: Non-Linear Least Squares (NLS)

This model is non-linear in its parameters. We will use an iterative numerical optimization method to find the parameter values that minimize the Residual Sum of Squares (RSS).

## 2.2 Assumptions

- **Normality of Errors:** The error term $e(t)$ is assumed to follow an i.i.d. normal distribution with a mean of zero and constant variance $\sigma^2$.
- **Non-Zero Denominator:** It is assumed that $\beta_0 + \beta_1 t \neq 0$ for all $t$ in the domain.

## 2.3 Estimation Algorithm

The parameters are estimated using R's `optim` function to perform Non-Linear Least Squares (NLS):

1. **Initial Guesses:** A simple linear model ($y \sim t$) is first fitted. The coefficients are used as initial guesses for $\alpha_0$ and $\alpha_1$. Initial guesses for $\beta_0$ and $\beta_1$ are set to 1.0 and 0.1.

2. **Optimization:** The `optim` function is called using the "L-BFGS-B" method to find the combination of parameters that minimizes the Sum of Squared Errors (SSE) function. Bounded constraints are used for stability.

3. **Time Variable:** This model is fit using the original time variable ($t_{orig}$).

```{r model2-fit}
# --- Model 2 Fit ---

# Use original time values
tt <- t_orig
yy <- y1
stopifnot(length(tt) == length(yy))

# Define model [a0, a1, b0, b1]
f_model_2 <- function(p, t) {
  a0 <- p[1]; a1 <- p[2]; b0 <- p[3]; b1 <- p[4]
  denom <- b0 + b1 * t
  if (any(abs(denom) < 1e-8)) return(rep(Inf, length(t)))
  (a0 + a1 * t) / denom
}

# Define SSE cost function
sse_2 <- function(p) {
  yhat <- f_model_2(p, tt)
  if (any(!is.finite(yhat))) return(1e10) 
  sum((yy - yhat)^2)
}

# Initial guess from simple linear fit
fit_poly_lin <- lm(yy ~ tt)
a0_init <- coef(fit_poly_lin)[1] 
a1_init <- coef(fit_poly_lin)[2] 
b0_init <- 1.0 
b1_init <- 0.1 
init_2 <- c(a0_init, a1_init, b0_init, b1_init)

# Use L-BFGS-B with bounds
fit_2 <- optim(init_2, sse_2, method = "L-BFGS-B",
               lower = c(-10, -10, 0.01, -10), 
               upper = c(10, 10, 20, 20),
               control = list(maxit = 2000))

params_2 <- fit_2$par
parameter_estimates_2 <- params_2
a0_2 <- params_2[1]
a1_2 <- params_2[2]
b0_2 <- params_2[3]
b1_2 <- params_2[4]

# Final predictions and residuals
fitted_values_2 <- f_model_2(params_2, tt)
residuals_2 <- yy - fitted_values_2
```

## Answers to the Questions for Model 2

### Ans 2.1: Least Squares Estimators

The estimated parameters for Model 2 are: **\(\hat{\alpha_0} = 1.87662 \), \(\hat{\alpha_1} = 0.09590313 \), \(\hat{\beta_0} = 0.3323027 \), and \(\hat{\beta_1} = 0.2812106\)**, calculated under the assumptions of normality, independence, and linearity of the error terms.

### Ans 2.2: Estimation Methodology for Least Squares Estimators

The parameters were estimated using the `optim` function (L-BFGS-B method). Initial guesses for $\alpha_0$ and $\alpha_1$ were derived from a standard `lm(y ~ t)` fit, while $\beta_0$ and $\beta_1$ were given starting values. The optimizer then iteratively minimized the RSS to find the final NLS estimates.

### Ans 2.3. Best Model : (See Ans 3.3)

```{r model2-accuracy}
# --- Model 2 Accuracy ---
rmse_2 <- sqrt(mean(residuals_2^2, na.rm = TRUE))
rss_2 <- sum(residuals_2^2)                  
tss_2 <- sum((yy - mean(yy))^2)  
r_squared_2 <- 1 - (rss_2 / tss_2)
k_2 <- 4
adjusted_r_squared_2 <- 1 - ((1 - r_squared_2) * (n - 1) / (n - k_2 - 1))
```

### Ans 2.4: Estimate of $\sigma^2$

The estimate of the error variance $\sigma^2$ is given by $\hat{\sigma}^2 = \frac{RSS}{n - k}$, where $n=60$ and $k=4$.The computed value of \(\hat{\sigma}^2\) is **0.06480029**.

```{r model2-fim-ci}
# Define the Jacobian for Model 2
jacobian_2 <- function(t, a0, a1, b0, b1) {
  denom <- b0 + b1 * t
  d_a0 <- 1 / denom
  d_a1 <- t / denom
  d_b0 <- -(a0 + a1 * t) / (denom^2)
  d_b1 <- -t * (a0 + a1 * t) / (denom^2)
  return(c(d_a0, d_a1, d_b0, d_b1))
}

# Fisher Information matrix calculation (4-param version)
fisher_information_4param <- function(t_vec, params, jac_func, sigma2) {
  n_obs <- length(t_vec)
  n_params <- length(params)
  J <- matrix(0, nrow = n_obs, ncol = n_params)
  
  for (i in 1:n_obs) {
    J[i, ] <- jac_func(t_vec[i], params[1], params[2], params[3], params[4])
  }
  
  fisher_matrix <- (1 / (n_obs * sigma2)) * (t(J) %*% J)
  lambda <- 1e-6
  fisher_matrix_regularized <- fisher_matrix + lambda * diag(n_params)
  
  return(fisher_matrix_regularized)
}

# Confidence Intervals calculation (4-param version)
confidence_intervals_4param <- function(fisher_matrix, parameter_estimates, alpha = 0.05) {
  fisher_inv <- ginv(fisher_matrix)
  se <- sqrt(diag(fisher_inv))
  z_value <- qnorm(1 - alpha / 2)
  ci_lower <- parameter_estimates - z_value * se
  ci_upper <- parameter_estimates + z_value * se
  return(list(lower = ci_lower, upper = ci_upper, se = se))
}

# Calculate sigma^2 estimate
sigma2_2 <- rss_2 / (n - k_2)

# Calculate Fisher Information Matrix
fisher_matrix_2 <- fisher_information_4param(tt, parameter_estimates_2, jacobian_2, sigma2_2)

# Calculate Confidence Intervals
ci_2 <- confidence_intervals_4param(fisher_matrix_2, parameter_estimates_2)

# Print the results
cat("--- Model 2 Results ---\n")
cat("Parameters (a0, a1, b0, b1):\n")
print(parameter_estimates_2)
cat("\nSigma^2 Estimate:\n")
print(sigma2_2)
cat("\nFisher Information Matrix:\n")
print(fisher_matrix_2)
cat("\nConfidence Intervals:\n")
print(ci_2)
```

### Ans 2.5: Confidence Intervals based on the Fisher Information Matrix

**Jacobian Matrix:**

$$J = \left[ \frac{1}{\beta_0 + \beta_1 t}, \frac{t}{\beta_0 + \beta_1 t}, \frac{-(\alpha_0 + \alpha_1 t)}{(\beta_0 + \beta_1 t)^2}, \frac{-(\alpha_0 + \alpha_1 t) \cdot t}{(\beta_0 + \beta_1 t)^2} \right]$$

### 2. Fisher Information Matrix (FIM)  
The Fisher Information Matrix (FIM) is computed as the sum of outer products of the Jacobian at each data point:

\[
\mathcal{I} =
\begin{bmatrix}
1.829990 & 9.508074 & -3.957474 & -10.77829 \\
9.508074 & 170.119944 & -10.778288 & -108.73125 \\
-3.957474 & -10.778288 & 10.203095 & 18.02859 \\
-10.778288 & -108.731251 & 18.028590 & 87.70463
\end{bmatrix}
\]

### 3. Confidence Intervals  
The 95% confidence intervals for each parameter are:

**\( \alpha_0 \)** (Intercept): \( [1.7655,\ 1.9877] \),  **\( \alpha_1 \)** (tt): \( [-0.4158,\ 0.6076] \),  **\( \beta_0 \)**: \( [-0.8457,\ 1.5103] \),  **\( \beta_1 \)**: \( [-0.5840,\ 1.1464] \).  

The corresponding standard errors are:  
**SE(\( \alpha_0 \))** = 0.0567,  **SE(\( \alpha_1 \))** = 0.2611,  **SE(\( \beta_0 \))** = 0.6010,  **SE(\( \beta_1 \))** = 0.4414.


### Ans 2.6 - 2.8: Residual Diagnostics and Model Fit

```{r model2-plots, fig.width=6, fig.height=4.5}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Residuals plot
plot(tt, residuals_2, main = "2.6: Residuals vs. Time (Model 2)",
     xlab = "Time (Original)", ylab = "Residuals", col = "black", pch = 19)
abline(h = 0, col = "red")

# Q-Q plot of residuals
qqnorm(residuals_2, main = "2.7(a): Q-Q Plot of Residuals (Model 2)")
qqline(residuals_2, col = "red")

# Histogram of residuals
hist(residuals_2, breaks = 10, main = "2.7(b): Histogram of Residuals (Model 2)",
     xlab = "Residuals", col = "lightblue", border = "black")

# Fitted values plot
tt_ord <- order(tt)
plot(tt[tt_ord], yy[tt_ord], type = "l", col = "blue", 
     main = "2.8: Observed vs. Fitted Data (Model 2)",
     xlab = "Time (Original)", ylab = "Values", lwd = 2)
lines(tt[tt_ord], fitted_values_2[tt_ord], col = "red", lwd = 2)
legend("topright", legend = c("Observed Data", "Model-2 Fit"), 
       col = c("blue", "red"), lty = 1, lwd = 2)

par(mfrow = c(1, 1))
```

```{r model2-ks-test, echo=FALSE,results='hide'}
# --- Model 2 KS Test ---
standardized_residuals_2 <- scale(residuals_2)
ks_test_2 <- ks.test(standardized_residuals_2, "pnorm", mean = 0, sd = 1)

cat("**Kolmogorov-Smirnov Test for Model 2:**\n\n")
cat("KS Test Statistic:", sprintf("%.6f", ks_test_2$statistic), "\n\n")
cat("KS Test p-value:", sprintf("%.6f", ks_test_2$p.value), "\n\n")
if (ks_test_2$p.value < 0.05) {
  cat("**Conclusion:** With p-value < 0.05, we reject the normality assumption for Model 2 residuals.\n")
} else {
  cat("**Conclusion:** With p-value ≥ 0.05, we fail to reject the normality assumption for Model 2 residuals.\n")
}
```
- **2.6: Residual Spread**: Residuals are well-centered around zero, indicating minimal bias and that the model’s predictions do not systematically deviate from observed values.

- **2.7: Kolmogorov-Smirnov Test**: With a test statistic of 0.091445 and a p-value of 0.663296, there’s no significant deviation from normality, suggesting that the residuals are likely normally distributed.

- **2.7(a & b): Q-Q Plot and Histogram**: The Q-Q plot and histogram(little bit) further confirm normality, as residuals align closely with the expected normal distribution.

- **2.8: Fitted Line**: The fitted line aligns closely with the observed data's central trend, indicating that Model 2 provides a good fit to the data.


# Model 3: Fourth-Degree Polynomial Regression Model

\( y_1 = \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 t^3 + \beta_4 t^4 + e(t) \), where \( e(t) \sim N(0, \sigma^2) \).

## 3.1 Theory: Polynomial Regression

This model captures the non-linear relationship between $y$ and $t$ by including polynomial terms up to the fourth degree. This is a form of multiple linear regression where the predictors are $t$, $t^2$, $t^3$, and $t^4$.

## 3.2 Assumptions

- **Normality of Errors:** It is assumed that the error term $e(t)$ follows an i.i.d. normal distribution with mean zero and constant variance $\sigma^2$.
- **Correct Model Form:** The model assumes a 4th-degree polynomial is the correct functional form.

## 3.3 Estimation Algorithm

1. **Ordinary Least Squares Estimation (OLSE)**: The model parameters \( \beta_0 \), \( \beta_1 \), \( \beta_2 \), \( \beta_3 \), and \( \beta_4 \) are estimated using OLSE, minimizing the residual sum of squares (RSS) between observed and fitted values. The estimated coefficients are derived from the fourth-degree polynomial fit.

2. **Fitted Values and Residual Analysis**: Fitted values are computed for each observation to evaluate the model's fit. Residuals are plotted against time to check for any systematic deviations from randomness, which would indicate potential issues with model specification.

3. **Plotting Observed vs. Fitted Data**: A plot of observed versus fitted values visually compares the model’s performance. This helps verify that the polynomial model aligns with the data trend.

```{r model3-fit}
# --- Model 3 Fit ---
# Fit a 4th-degree polynomial regression model using ORIGINAL t
model3 <- lm(y1 ~ poly(t_orig, 4, raw = TRUE))

# Calculate fitted values and residuals
fitted_values_3 <- fitted(model3)
residuals_3 <- resid(model3)
parameter_estimates_3 <- coef(model3)
```

## Answers to the Questions for Model 3

### Ans 3.1: Least Squares Estimators under Model 3

The estimated parameters for the 4th-Degree Polynomial Regression Model are:  
**\(\hat{\beta_0} = 3.048821\), \(\hat{\beta_1} = -0.3805117 \), \(\hat{\beta_2} = 0.01988504\), \(\hat{\beta_3} = -0.0004233991\), and \(\hat{\beta_4} = 0.000003125622 \)**, derived under the standard assumptions .

### Ans 3.2: Estimation Methodology for Least Squares Estimators

The least squares estimates for parameters \(\beta_0\), \(\beta_1\), \(\beta_2\), \(\beta_3\), and \(\beta_4\) were obtained using ordinary least squares (OLS) regression, fitting the observed data to a polynomial of degree 4. The R function `lm()` was used to perform the regression, automatically transforming the predictor variable \( t \) into polynomial terms.

### Ans 3.3. Best Fitted Model

| Model                         | RSS       | R²        | Adjusted R² | KS Test p-value |
|-------------------------------|------------|-----------|--------------|-----------------|
| Model 1 (Exponential)         | 3.283324   | 0.810364  | 0.792806     | 0.713978        |
| Model 2 (Rational [1/1])      | 3.628816   | 0.790410  | 0.775167     | 0.663296        |
| Model 3 (Poly 4th degree)     | 3.777512   | 0.781821  | 0.761620     | 0.968311        |

Model 1 (**Exponential**) performs the best, with the **highest R² (0.810364)** and **Adjusted R² (0.792806)**, along with a **reasonably low RSS (3.283324)** and a **strong KS Test p-value (0.713978)**, indicating a good overall model fit.


```{r model3-accuracy}
# --- Model 3 Accuracy ---
rmse_3 <- sqrt(mean(residuals_3^2, na.rm = TRUE))
rss_3 <- sum(residuals_3^2)                  
tss_3 <- sum((y1 - mean(y1))^2)  
r_squared_3 <- 1 - (rss_3 / tss_3)
k_3 <- 5
adjusted_r_squared_3 <- 1 - ((1 - r_squared_3) * (n - 1) / (n - k_3 - 1))
```

### Ans 3.4: Estimate of $\sigma^2$

The estimate of the error variance $\sigma^2$ is given by $\hat{\sigma}^2 = \frac{RSS}{n - k}$, where \( n \) is the number of observations, and \( p \) is the number of estimated parameters (including the intercept). The computed value of \(\hat{\sigma}^2\) for Model 3 is **0.06868204**.

```{r model3-fim-ci}
# Define the Jacobian of the model
jacobian_3 <- function(t, beta0, beta1, beta2, beta3, beta4) {
  d_beta0 <- 1 
  d_beta1 <- t  
  d_beta2 <- t^2  
  d_beta3 <- t^3  
  d_beta4 <- t^4  
  return(c(d_beta0, d_beta1, d_beta2, d_beta3, d_beta4))
}

# Fisher Information matrix calculation (5-param version)
fisher_information_5param <- function(t_vec, params, jac_func, sigma2) {
  n_obs <- length(t_vec)
  n_params <- length(params)
  J <- matrix(0, nrow = n_obs, ncol = n_params)
  
  for (i in 1:n_obs) {
    J[i, ] <- jac_func(t_vec[i], params[1], params[2], params[3], params[4], params[5])
  }
  
  fisher_matrix <- (1 / (n_obs * sigma2)) * (t(J) %*% J)
  lambda <- 1e-6
  fisher_matrix_regularized <- fisher_matrix + lambda * diag(n_params)
  
  return(fisher_matrix_regularized)
}

# Confidence Intervals calculation (5-param version)
confidence_intervals_5param <- function(fisher_matrix, parameter_estimates, alpha = 0.05) {
  fisher_inv <- ginv(fisher_matrix)
  se <- sqrt(diag(fisher_inv))
  z_value <- qnorm(1 - alpha / 2)
  ci_lower <- parameter_estimates - z_value * se
  ci_upper <- parameter_estimates + z_value * se
  return(list(lower = ci_lower, upper = ci_upper, se = se))
}

# Calculate sigma^2 estimate
sigma2_3 <- rss_3 / (n - k_3)

# Calculate Fisher Information Matrix using t_orig
fisher_matrix_3 <- fisher_information_5param(t_orig, parameter_estimates_3, jacobian_3, sigma2_3)

# Calculate Confidence Intervals
ci_3 <- confidence_intervals_5param(fisher_matrix_3, parameter_estimates_3)

# Print the results
cat("--- Model 3 Results ---\n")
cat("Parameters (beta0, beta1, beta2, beta3, beta4):\n")
print(parameter_estimates_3)
cat("\nSigma^2 Estimate:\n")
print(sigma2_3)
cat("\nFisher Information Matrix:\n")
print(fisher_matrix_3)
cat("\nConfidence Intervals:\n")
print(ci_3)
```

### Ans 3.5: Confidence Intervals based on the Fisher Information Matrix

**Jacobian Matrix:**

$$J = \begin{bmatrix} 1 & t & t^2 & t^3 & t^4 \end{bmatrix}$$

### 2. Fisher Information Matrix (FIM)  
The Fisher Information Matrix (FIM) is computed as the sum of outer products of the Jacobian at each data point:

\[
\mathcal{I} =
\begin{bmatrix}
1.455985e+01 & 4.440754e+02 & 1.791104e+04 & 8.126579e+05 & 3.932906e+07 \\
4.440754e+02 & 1.791104e+04 & 8.126579e+05 & 3.932906e+07 & 1.982614e+09 \\
1.791104e+04 & 8.126579e+05 & 3.932906e+07 & 1.982614e+09 & 1.027987e+11 \\
8.126579e+05 & 3.932906e+07 & 1.982614e+09 & 1.027987e+11 & 5.441037e+12 \\
3.932906e+07 & 1.982614e+09 & 1.027987e+11 & 5.441037e+12 & 2.925536e+14
\end{bmatrix}
\]

### 3. Confidence Intervals  
The 95% confidence intervals for each parameter are:

**\( \beta_0 \)**: \( [3.048821,\ 3.048821] \),  **\( \beta_1 \)**: \( [-3.805118 \times 10^{-1},\ -3.805117 \times 10^{-1}] \),  **\( \beta_2 \)**: \( [1.988289 \times 10^{-2},\ 1.988720 \times 10^{-2}] \),  **\( \beta_3 \)**: \( [-4.722314 \times 10^{-4},\ -3.745551 \times 10^{-4}] \),  **\( \beta_4 \)**: \( [2.209250 \times 10^{-6},\ 4.041993 \times 10^{-6}] \).  

The corresponding standard errors are:  
**SE(\( \beta_0 \))** = 1.262351e−09,  **SE(\( \beta_1 \))** = 3.816825e−08,  **SE(\( \beta_2 \))** = 1.099094e−06,  **SE(\( \beta_3 \))** = 2.492087e−05,  **SE(\( \beta_4 \))** = 4.675453e−07.

   

### Ans 3.6 - 3.8: Residual Diagnostics and Model Fit

```{r model3-plots, fig.width=6, fig.height=4.5}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Residuals Plot for Model 3
plot(t_orig, residuals_3, main = "3.6: Residuals vs. Time (Model 3)",
     xlab = "Time (Original)", ylab = "Residuals", col = "black", pch = 19)
abline(h = 0, col = "red")

# Q-Q Plot for Model 3 Residuals
qqnorm(residuals_3, main = "3.7(a): Q-Q Plot of Residuals (Model 3)")
qqline(residuals_3, col = "red")  

# Histogram of Residuals for Model 3
hist(residuals_3, breaks = 10, main = "3.7(b): Histogram of Residuals (Model 3)",
     xlab = "Residuals", col = "lightblue", border = "black")

# Observed vs. Fitted Data Plot for Model 3
plot(t_orig, y1, type = "l", col = "blue", main = "3.8: Observed vs. Fitted Data (Model 3)",
     xlab = "Time (Original)", ylab = "Values", lwd = 2)
lines(t_orig, fitted_values_3, col = "red", lwd = 2)
legend("topright", legend = c("Observed Data", "Model-3 Fit"), 
       col = c("blue", "red"), lty = 1, lwd = 2)

par(mfrow = c(1, 1))
```

```{r model3-ks-test, echo=FALSE,results='hide'}
# --- Model 3 KS Test ---
standardized_residuals_3 <- scale(residuals_3)
ks_test_3 <- ks.test(standardized_residuals_3, "pnorm", mean = 0, sd = 1)

cat("**Kolmogorov-Smirnov Test for Model 3:**\n\n")
cat("KS Test Statistic:", sprintf("%.6f", ks_test_3$statistic), "\n\n")
cat("KS Test p-value:", sprintf("%.6f", ks_test_3$p.value), "\n\n")
if (ks_test_3$p.value < 0.05) {
  cat("**Conclusion:** The Kolmogorov-Smirnov test statistic is ", sprintf("%.6f", ks_test_3$statistic), 
      " with a p-value of ", sprintf("%.6f", ks_test_3$p.value), 
      ". With p-value < 0.05, we reject the normality assumption for Model 3 residuals.\n")
} else {
  cat("**Conclusion:** The Kolmogorov-Smirnov test statistic is ", sprintf("%.6f", ks_test_3$statistic), 
      " with a p-value of ", sprintf("%.6f", ks_test_3$p.value), 
      ". With p-value ≥ 0.05, we fail to reject the normality assumption for Model 3 residuals.\n")
}
```


- **3.6: Residual Spread**: The residuals are well-centered around zero, indicating minimal bias in the model's predictions.

- **3.7: Kolmogorov-Smirnov Test**: The Kolmogorov-Smirnov test statistic for the residuals is **0.061133** with a p-value of **0.968311**. Since the p-value is greater than 0.05, there is no significant deviation from normality, suggesting that the residuals follow a normal distribution.

- **3.7(a & b): Q-Q Plot and Histogram**: Both the Q-Q plot and the histogram of the residuals further confirm the normality assumption, as the residuals align closely with a normal distribution.

- **3.8: Fitted Line**: The fitted polynomial curve closely matches the observed data's central trend, indicating that the 4th-degree polynomial model provides a good fit to the data.

```{r model-comparison, echo=FALSE,results='hide'}
# Create a comparison table
model_names <- c("Model 1 (Exponential)", "Model 2 (Rational [1/1])", "Model 3 (Poly 4th deg)")
rss_values <- c(rss_1, rss_2, rss_3)
r_squared_values <- c(r_squared_1, r_squared_2, r_squared_3)
adj_r_squared_values <- c(adjusted_r_squared_1, adjusted_r_squared_2, adjusted_r_squared_3)
ks_p_values <- c(ks_test_1$p.value, ks_test_2$p.value, ks_test_3$p.value)

comparison_df <- data.frame(
  Model = model_names,
  RSS = rss_values,
  R_Squared = r_squared_values,
  Adjusted_R_Squared = adj_r_squared_values,
  KS_Test_p_value = ks_p_values
)

cat("## Model Fit Comparison\n\n")
knitr::kable(comparison_df, digits = 6, caption = "Comparison of Model Fit Statistics")

cat("\n\n### Interpretation Guide\n\n")
cat("* **RSS (Residual Sum of Squares):** Lower is better. Indicates better fit with smaller residual errors.\n\n")
cat("* **R-Squared:** Higher is better (ranges from 0 to 1). Represents the proportion of variance explained by the model.\n\n")
cat("* **Adjusted R-Squared:** Higher is better. Adjusts R-Squared for the number of parameters to prevent overfitting.\n\n")
cat("* **KS Test p-value:** A value < 0.05 suggests the residuals are *not* normally distributed, which violates a model assumption. Values ≥ 0.05 indicate residuals are approximately normally distributed.\n\n")

# Determine best model
best_model_idx <- which.min(rss_values)
cat("\n### Conclusion\n\n")
cat(sprintf("Based on the comparison metrics, **%s** appears to be the best fitted model with the lowest RSS of %.6f and the highest Adjusted R-Squared of %.6f.", 
            model_names[best_model_idx], rss_values[best_model_idx], adj_r_squared_values[best_model_idx]))
```

